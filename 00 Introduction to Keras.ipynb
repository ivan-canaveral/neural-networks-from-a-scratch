{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with Keras Squential model\n",
    "\n",
    "The Sequential model is a linear stack of layers.\n",
    "\n",
    "You can create a Sequential model by passing a list of layer instances to the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also simply add layers via the .add() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the input shape \n",
    "\n",
    "The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:\n",
    "\n",
    "* Pass an `input_shape` argument to the first layer. This is a shape tuple (a tuple of integers or `None` entries, where `None` indicates that any positive integer may be expected). In `input_shape`, the batch dimension is not included.\n",
    "\n",
    "* Some 2D layers, such as `Dense`, support the specification of their input shape via the argument `input_dim`, and some 3D temporal layers support the arguments `input_dim` and `input_length`.\n",
    "\n",
    "* If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a  `batch_size` argument to a layer. If you pass both `batch_size=32` and `input_shape=(6, 8)` to a layer, it will then expect every batch of inputs to have the batch shape `(32, 6, 8)`.\n",
    "\n",
    "As such, the following snippets are strictly equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the `compile` method. It receives three arguments:\n",
    "\n",
    "* An optimizer. This could be the string identifier of an existing optimizer (such as `rmsprop` or `adagrad`), or an instance of the `Optimizer` class. (In this case, Optimizer means a method or algorithm that optimizes some functions)\n",
    "\n",
    "* A loss function. This is the objective function that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function.\n",
    "\n",
    "* A list of metrics. For any classification problem you will want to set this to `metrics=['accuracy']`. A metric could be the string identifier of an existing metric or a custom metric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "\n",
    "# For custom metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', mean_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Keras models are trained on a Numpy arrays of input data and labels. For training a mode, you will typically use the `fit` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fit function\n",
    "\n",
    "This is a little break about the fit function. The structure of this function is somethin like this:\n",
    "\n",
    "`fit(self, x, y, batch_size=32, epochs=10, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)`\n",
    "\n",
    "This trains the model for a fixed number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Arguments\n",
    "\n",
    "* *x*: input data, as a Numpy array or list of Numpy arrays (if the model has multiple inputs).\n",
    "* *y*: labels, as a Numpy array.\n",
    "* *batch_size*: integer. Number of samples per gradient update.\n",
    "* *epochs*: integer, the number of epochs to train the model.\n",
    "* *verbose*: 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.\n",
    "* *callbacks*: list of keras.callbacks.Callback instances. List of callbacks to apply during training.\n",
    "* *validation_split*: float (0. < x < 1). Fraction of the data to use as held-out validation data.\n",
    "* *validation_data*: tuple (x_val, y_val) or tuple (x_val, y_val, val_sample_weights) to be used as held-out validation data. Will override validation_split.\n",
    "* *shuffle*: boolean or str (for 'batch'). Whether to shuffle the samples at each epoch. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
    "* *class_weight*: dictionary mapping classes to a weight value, used for scaling the loss function (during training only).\n",
    "* *sample_weight*: Numpy array of weights for the training samples, used for scaling the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile().\n",
    "* *initial_epoch*: epoch at which to start training (useful for resuming a previous training run)\n",
    "\n",
    "#####Â Returns\n",
    "\n",
    "A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "\n",
    "##### Raises\n",
    "\n",
    "RuntimeError: if the model was never compiled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict \n",
    "\n",
    "`predict(self, x, batch_size=32, verbose=0)``\n",
    "\n",
    "Generates output predictions for the input samples.\n",
    "\n",
    "The input samples are processed batch by batch.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "x: the input data, as a Numpy array.\n",
    "batch_size: integer.\n",
    "verbose: verbosity mode, 0 or 1.\n",
    "\n",
    "#### Returns\n",
    "\n",
    "A Numpy array of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples\n",
    "\n",
    "Now we're going to see a couple of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': [0.51000000000000001, 0.54700000000000004, 0.53200000000000003, 0.55700000000000005, 0.55800000000000005, 0.56299999999999994, 0.56599999999999995, 0.56899999999999995, 0.57499999999999996, 0.57199999999999995], 'loss': [0.70387321805953984, 0.69076384258270263, 0.69247787952423101, 0.68751943874359134, 0.68315855884552001, 0.68294048547744746, 0.67790679168701173, 0.67390061044692995, 0.67202281093597416, 0.66684537315368653]}\n"
     ]
    }
   ],
   "source": [
    "### Example 1\n",
    "# For a single-input model with 2 classes (binary classification):\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "#print(\"Input data\")\n",
    "#print(data)\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "#print(\"\\nLabels\")\n",
    "#print(labels)\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "hist = model.fit(data, labels, verbose = 0, epochs=10, batch_size=32)\n",
    "print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.60028714]\n",
      " [ 0.43608826]\n",
      " [ 0.49870765]\n",
      " [ 0.45481431]\n",
      " [ 0.4793022 ]]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(np.random.random((5, 100)))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example 2\n",
    "# For a single-input model with 10 classes (categorical classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "import keras\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "#print(\"Labels\")\n",
    "#print(labels)\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "#print(\"one-hot labels\")\n",
    "#print(one_hot_labels)\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "hist = model.fit(data, one_hot_labels, verbose=0, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07852356  0.09424989  0.09305289  0.11304904  0.1484883   0.1208453\n",
      "   0.10267175  0.05804131  0.07525445  0.11582354]\n",
      " [ 0.07616915  0.12711795  0.07674219  0.0805225   0.15082555  0.09188802\n",
      "   0.11951567  0.13229284  0.07625939  0.06866679]\n",
      " [ 0.11161524  0.12163014  0.06704808  0.092452    0.09380397  0.09955897\n",
      "   0.11215826  0.12134843  0.08861399  0.09177095]\n",
      " [ 0.08985351  0.05642612  0.08618461  0.05884392  0.09129987  0.14388478\n",
      "   0.13352601  0.12331535  0.06924877  0.14741705]\n",
      " [ 0.07444849  0.09991132  0.09586181  0.09125136  0.12575373  0.12258089\n",
      "   0.08111702  0.09624729  0.10771045  0.1051176 ]]\n"
     ]
    }
   ],
   "source": [
    "data = np.random.random((5,100))\n",
    "#one_hot_data = keras.utils.to_categorical(data, num_classes = 10)\n",
    "prediction = model.predict(data)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
